Documentación y Testing
-----------------------

    "Si no está en el manual está equivocado. Si está en el manual
    es redundante."

    -- Califa Omar, Alejandría, Año 634.

¿Pero cómo sabemos si el programa hace *exactamente* lo que dice el manual?

Bueno, pues *para eso* (entre otras cosas) están los tests [#]_. Los tests son
la rama militante de la documentación. La parte activa que se encarga de que ese manual no sea letra muerta e ignorada por perder contacto con la realidad,
si no un texto que refleja lo que realmente existe.

.. [#] También están para la gente mala que no documenta.

Si la realidad (el funcionamiento del programa) se aparta del ideal (el manual),
es el trabajo del test chiflar y avisar que está pasando. Para que esto sea
efectivo tenemos que cumplir varios requisitos:

* Los tests tienen que poder detectar todos los errores. (cobertura)

* Los tests tienen que ser ejecutados ante cada cambio, y las
  diferencias de resultado explicadas. (integración)

* El programador y el documentador y el tester (o sea uno) tiene que
  aceptar que hacer tests es necesario. Si se lo ve como una carga, no vale
  la pena: vas a aprender a ignorar las fallas, a hacer "pasar" los tests, a
  no hacer tests de las cosas que sabés que son difíciles. (ganas)

Por suerte en Python hay muchas herramientas que hacen que testear sea, si no divertido, por lo menos tolerable.


Doctests
~~~~~~~~

    "Los comentarios mienten. El código no."

    -- Ron Jeffries


Un comentario mentiroso es peor que ningún comentario. Y los comentarios se
vuelven mentira porque el código cambia y nadie edita los comentarios. Es
el problema de repetirse, uno ya dijo lo que quería en el código, y tiene que
volver a explicarlo en un comentario, a la larga las copias divergen,
y siempre el que está equivocado es el comentario.

Un doctest permite **asegurar** que el comentario es cierto, porque el
comentario tiene código de su lado.

Tomemos un ejemplo zonzo: una función para traducir al rosarino [#]_.

.. [#] Este ejemplo surgió de una discusión de PyAr. El código que contiene es
   tal vez un poco denso. No te asustes, lo importante no es el código, si no
   lo que hay alrededor.

.. admonition:: Lenguaje Rosarino

   Inventado (o popularizado) por Alberto Olmedo, el rosarino es un lenguaje en
   el cual la vocal acentuada X se reemplaza por XgasX con el acento al final
   (á por agasá, e por egasé, etc).

   Algunos ejemplos:

   rosarino => rosarigasino
   
   té => té (no se expanden monosílabos)
   
   brújula => brugasújula

   queso => quegaseso


Aquí tenemos una primera versión, que funciona sólo en palabras con acento ortográfico:

.. class:: titulo-listado

gaso1.py

.. class:: listado

.. code-block:: python
   :linenos:
   :include: gaso1.py


Y acá viene la primera cosa importante de testing. Uno quiere testear todos los
comportamientos deseados del código.

Si el código se supone que ya hace algo bien, aunque sea algo muy chiquitito, es
el momento ideal para empezar a hacer testing. Si vas a esperar a que la función
sea "interesante", ya va a ser muy tarde. Vas a tener un déficit de tests, vas
a tener que ponerte un día sólo a escribir tests, y es aburrido.

¿Como sé yo que esa regexp hace lo que yo quiero? ¡Porque la probé! Como no soy
el mago de las expresiones regulares que las saca de la galera, hice esto en el
intérprete interactivo (reemplazo la funcion ``gas`` con una versión boba):

.. code-block:: pycon

    >>> import re
    >>> palabra=u'cámara'
    >>> print re.sub(u'([\xe1\xe9\xed\xf3\xfa])',
    ...     lambda x: x.group(0)+'gas'+x.group(0),palabra,1)
    
    cágasámara

¿Y como sé que la función ``gas`` hace lo que quiero? Porque hice esto:

.. code-block:: pycon

    >>> import unicodedata
    >>> def gas(letra):
    ...     return u'%sgas%s'%(unicodedata.normalize('NFKD',
    ...         letra).encode('ASCII', 'ignore'), letra)
    >>> print gas(u'á')
    agasá
    >>> print gas(u'a')
    agasa


Si no hubiera hecho ese test manual no tendría la más mínima confianza en este
código, y creo que casi todos hacemos esta clase de cosas, ¿o no?.

El problema con este testing manual ad-hoc es que lo hacemos una vez, la función
hace lo que se supone debe hacer, y nos olvidamos.

Por suerte *no tiene porqué ser así*, gracias a los doctests.

De hecho, el doctest es poco más que cortar y pegar esos tests informales que
mostré arriba. Veamos la versión con doctests:

.. class:: titulo-listado

gaso2.py

.. class:: listado

.. code-block:: python
   :linenos:
   :include: gaso2.py

Eso es todo lo que se necesita para implementar doctests. ¡En serio!. ¿Y cómo
hago para saber si los tests pasan o fallan? Hay muchas maneras. Tal vez la
que más me gusta es usar `Nose <http://somethingaboutorange.com/mrl/projects/nose/>`_ 
una herramienta cuyo único objetivo es hacer que testear sea fácil.

::

    [ralsina@hp python-no-muerde]$ nosetests --with-doctest -v gaso2.py
    Doctest: gaso2.gas ... ok
    Doctest: gaso2.gasear ... ok

    ----------------------------------------------------------------------
    Ran 2 tests in 0.035s

    OK
    

Lo que hizo nose es "descubrimiento de tests" (test discovery). Toma la carpeta
actual o el archivo que indiquemos (en este caso ``gaso2.py``, encuentra las
cosas que parecen tests y las usa. El parámetro ``--with-doctest`` es para que reconozca doctests (por default los ignora), y el ``-v``
es para que muestre cada cosa que prueba.

De ahora en más, cada vez que el programa se modifique, volvemos a correr el test suite (eso significa "un conjunto de tests"). Si falla alguno que antes andaba,
es una regresión, paramos de romper y la arreglamos. Si pasa alguno que antes
fallaba, es un avance, nos felicitamos y nos damos un caramelo.

Dentro del limitado alcance de nuestro programa actual, lo que hace,
lo hace bien. Obviamente hay muchas cosas que hace mal:

.. code-block:: pycon

    >>> import gaso2
    >>> gaso2.gasear('rosarino')
    'rosarino'
    >>> print 'OH NO!'
    'OH NO!'

¿Qué hacemos entonces? ¡Agregamos un test que falla! Bienvenido al mundo del TDD
o "Desarrollo impulsado por tests" (Test Driven Development). La idea es que,
en general, si sabemos que hay un bug, seguimos este proceso:

* Creamos un test que falla.
* Arreglamos el código para que no falle el test.
* Verificamos que no rompimos otra cosa usando el test suite.

Un test que falla es **bueno** porque nos marca qué hay que corregir. Si los
tests son buenos, y cada uno prueba una sola cosa, entonces hasta nos va a
indicar qué parte del código es la que está rota.

Entonces, el problema de ``gaso2.py`` es que no funciona cuando no hay acentos
ortográficos. ¿Solución? Una función que diga donde está el acento prosódico en
una palabra [#]_

.. [#] Y en este momento agradezcan que esto es castellano, que es un idioma
       casi obsesivo compulsivo en su regularidad.

Modificamos ``gasear`` así:

.. class:: titulo-listado

gaso3.py

.. class:: listado

.. code-block:: python
   :linenos:
   :include: gaso3.py
   :start-at: def gasear(palabra):

Y esta es la funcion nueva:

.. class:: titulo-listado

buscaacento1.py

.. class:: listado

.. code-block:: python
   :linenos:
   :include: buscaacento1.py

¿Notaste que agregar tests de esta forma no se siente como una carga?

Es parte natural de escribir el código, pienso, "uy, esto no debe andar", meto
el test como creo que debería ser en el docstring, y de ahora en más sé si eso
anda o no.

Por otro lado te da la tranquilidad de "no estoy rompiendo nada". Por lo menos
nada que no estuviera funcionando exclusivamente por casualidad.

Por ejemplo, ``gaso1.py`` pasaría el test de la palabra 'la' y ``gaso2.py``
fallaría, pero no porque ``gaso1.py`` estuviera haciendo algo bien, si no
porque respondía de forma afortunada.

